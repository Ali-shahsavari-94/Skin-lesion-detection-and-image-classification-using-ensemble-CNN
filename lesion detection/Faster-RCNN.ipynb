{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "green-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from random import shuffle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers import Flatten, Dense, Dropout,BatchNormalization,Activation, Convolution2D, add\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate\n",
    "from keras import optimizers, regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializers, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-identifier",
   "metadata": {},
   "source": [
    "## preparation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDIR='/main image/train'\n",
    "CATEGORIES = ['basal', 'melanoma']\n",
    "IMG_SIZE=256\n",
    "training_data=[]\n",
    "\n",
    "def creat_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path= os.path.join(TRAINDIR,category)\n",
    "        class_num=CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img))#,cv2.IMREAD_GRAYSCALE)\n",
    "                #new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                training_data.append([img_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "creat_training_data()\n",
    "\n",
    "print(len(training_data))\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:10]:\n",
    "    print(sample[1])\n",
    "    \n",
    "X_TRAIN=[]\n",
    "Y_TRAIN=[]\n",
    "\n",
    "for features, label in training_data :\n",
    "    X_TRAIN.append(features)\n",
    "    Y_TRAIN.append(label)\n",
    "\n",
    "X_TRAIN=np.array(X_TRAIN).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "print(X_TRAIN.shape)\n",
    "\n",
    "\n",
    "test_data=[]\n",
    "TESTDIR='/main image/test'\n",
    "#TESTDIR='C:/Users/Asus/Music/ali/main image/prods/test'\n",
    "CATEGORIES = ['basal', 'melanoma']\n",
    "IMG_SIZE=256\n",
    "\n",
    "def creat_test_data():\n",
    "    for category in CATEGORIES:\n",
    "        path= os.path.join(TESTDIR,category)\n",
    "        class_num=CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img))#,cv2.IMREAD_GRAYSCALE)\n",
    "                #new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "                test_data.append([img_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "creat_test_data()\n",
    "print(len(test_data))\n",
    "\n",
    "random.shuffle(test_data)\n",
    "for sample in test_data[:10]:\n",
    "    print(sample[1])\n",
    "\n",
    "\n",
    "X_TEST=[]\n",
    "Y_TEST=[]\n",
    "\n",
    "for features , label in test_data :\n",
    "    X_TEST.append(features)\n",
    "    Y_TEST.append(label)\n",
    "\n",
    "X_TEST=np.array(X_TEST).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "print(X_TEST.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode=LabelEncoder()\n",
    "encode.fit(Y_TRAIN)\n",
    "encoded_Y = encode.transform(Y_TRAIN)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "\n",
    "encodeT=LabelEncoder()\n",
    "encodeT.fit(Y_TEST)\n",
    "encoded_T = encodeT.transform(Y_TEST)\n",
    "dummy_T = np_utils.to_categorical(encoded_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-lancaster",
   "metadata": {},
   "source": [
    "# ROI pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "\n",
    "        self.dim_ordering = K.common.image_dim_ordering()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = K.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "                \n",
    "\n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-bookmark",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classified-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes        = 2\n",
    "batch_size         = 32         # 64 or 32 or other\n",
    "epochs             = 3\n",
    "iterations         = 1       \n",
    "USE_BN=True\n",
    "DROPOUT=0.2 # keep 80%\n",
    "CONCAT_AXIS=3\n",
    "weight_decay=1e-4\n",
    "DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stopped-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch < 100:\n",
    "        return 0.01\n",
    "    if epoch < 200:\n",
    "        return 0.001\n",
    "    return 0.0001\n",
    "\n",
    "def conv_block(x, nb_filter, nb_row, nb_col, border_mode='same', subsample=(1,1), bias=False):\n",
    "    x = Convolution2D(nb_filter, nb_row, nb_col, subsample=subsample, border_mode=border_mode, bias=bias,\n",
    "                     init=\"he_normal\",dim_ordering='tf',W_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_stem(img_input,concat_axis):\n",
    "    x = Conv2D(32,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(img_input)\n",
    "    x = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x)) #256*256*32\n",
    "    \n",
    "    x = Conv2D(32,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x))#256*256*32\n",
    "    \n",
    "    x = Conv2D(64,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x))#128*128*64\n",
    "    \n",
    "    x = Conv2D(64,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x))#128*128*64\n",
    "    # stem1\n",
    "    x_1 = Conv2D(128,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x_1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_1))#64*64*128\n",
    "    \n",
    "    x_2 = Conv2D(64,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x_2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_2))#128*128*64\n",
    "    x_2 = MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x_2)#64*64*256\n",
    "    \n",
    "    x = concatenate([x_1,x_2],axis=concat_axis) #<- 64*64 *192\n",
    "    # stem2\n",
    "    x_1 = Conv2D(64,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x_1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_1)) #64*64*64\n",
    "    x_1 = Conv2D(128,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x_1)\n",
    "    x_1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_1))#64*64*128\n",
    "    \n",
    "    x_2 =  Conv2D(128,kernel_size=(1,1),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x_2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_2))#64*64*128\n",
    "    #x_2 = conv_block(x_2,64,1,7)\n",
    "    #x_2 = conv_block(x_2,64,7,1)\n",
    "    x_2 =  Conv2D(256,kernel_size=(3,3),strides=(1,1),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x_2) \n",
    "    x_2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_2))#64*64*256\n",
    "    x = concatenate([x_1,x_2],axis=concat_axis)\n",
    "    # stem3 <- 64*64 *384\n",
    "    x_1 = Conv2D(128,kernel_size=(3,3),strides=(2,2),padding='same',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    x_1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(x_1))#32*32*128\n",
    "    x_2 = MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x) #32*32 *384\n",
    "    x = concatenate([x_1,x_2],axis=concat_axis) #<- 32*32*512\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developed-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_A(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,scale_residual=False,use_bias=True,kernel_initializer=\"he_normal\",bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,weight_decay=weight_decay):\n",
    "    (branch1,branch2,branch3,branch4)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    #64*64*384\n",
    "    #1x1\n",
    "    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1))\n",
    "    #1x1->3x3\n",
    "    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    #1x1->3x3->3x3\n",
    "    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    pathway3=Conv2D(filters=branch3[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    pathway3=Conv2D(filters=branch3[2],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    # concatenate\n",
    "    pathway_123 = concatenate([pathway1,pathway2,pathway3],axis=concat_axis)\n",
    "    pathway_123 = Conv2D(branch4[0],kernel_size=(1,1),strides=(1,1),padding='same',activation = 'linear',\n",
    "                         kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(pathway_123)\n",
    "    if scale_residual: \n",
    "        x = Lambda(lambda p: p * 0.1)(x)\n",
    "    return add([x,pathway_123])\n",
    "\n",
    "def reduce_A(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,use_bias=True,kernel_initializer=\"he_normal\",bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,weight_decay=weight_decay):\n",
    "    (branch1,branch2)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    #1x1\n",
    "    pathway1 = Conv2D(filters=branch1[0],kernel_size=(3,3),strides=2,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1))\n",
    "    #1x1->3x3->3x3\n",
    "    pathway2 = Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    pathway2 = Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    pathway2 = Conv2D(filters=branch2[2],kernel_size=(3,3),strides=2,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    # max pooling\n",
    "    pathway3 = MaxPooling2D(pool_size=(3,3),strides=2,padding=padding,data_format=DATA_FORMAT)(x)\n",
    "    return concatenate([pathway1,pathway2,pathway3],axis=concat_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "devoted-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_B(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,scale_residual=False,use_bias=True,kernel_initializer=\"he_normal\",bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,weight_decay=weight_decay):\n",
    "    (branch1,branch2,branch3)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    #16*16*1536 \n",
    "    #1x1\n",
    "    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1))\n",
    "    #1x1->1x7->7x1\n",
    "    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    pathway2 = conv_block(pathway2,branch2[1],1,7)\n",
    "    pathway2 = conv_block(pathway2,branch2[2],7,1)\n",
    "    # concatenate\n",
    "    pathway_12 = concatenate([pathway1,pathway2],axis=concat_axis)\n",
    "    pathway_12 = Conv2D(branch3[0],kernel_size=(1,1),strides=(1,1),padding='same',activation = 'linear',\n",
    "                        kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(pathway_12)\n",
    "    if scale_residual: \n",
    "        x = Lambda(lambda p: p * 0.1)(x)\n",
    "    return add([x,pathway_12])\n",
    "\n",
    "def reduce_B(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,use_bias=True,kernel_initializer=\"he_normal\",bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,weight_decay=weight_decay):\n",
    "    (branch1,branch2,branch3)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    #1x1->3x3\n",
    "    pathway1 = Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1)) \n",
    "    pathway1 = Conv2D(filters=branch1[1],kernel_size=(3,3),strides=2,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway1)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1))\n",
    "    #1x1->3x3\n",
    "    pathway2 = Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2)) \n",
    "    pathway2 = Conv2D(filters=branch2[1],kernel_size=(3,3),strides=2,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    #1x1->3x3->3x3\n",
    "    pathway3 = Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    pathway3 = Conv2D(filters=branch3[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    pathway3 = Conv2D(filters=branch3[2],kernel_size=(3,3),strides=2,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)\n",
    "    pathway3 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway3))\n",
    "    # max pooling\n",
    "    pathway4 = MaxPooling2D(pool_size=(3,3),strides=2,padding=padding,data_format=DATA_FORMAT)(x)\n",
    "    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reduced-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_C(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,scale_residual=False,use_bias=True,kernel_initializer=\"he_normal\",bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,weight_decay=weight_decay):\n",
    "    (branch1,branch2,branch3)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    8*8*3072\n",
    "    #1x1\n",
    "    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway1 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway1))\n",
    "    #1x1->1x3->3x1\n",
    "    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)\n",
    "    pathway2 = Activation('relu')(BatchNormalization(momentum=0.9, epsilon=1e-5)(pathway2))\n",
    "    pathway2 = conv_block(pathway2,branch2[1],1,3)\n",
    "    pathway2 = conv_block(pathway2,branch2[2],3,1)\n",
    "    # concatenate\n",
    "    pathway_12 = concatenate([pathway1,pathway2],axis=concat_axis)\n",
    "    pathway_12 = Conv2D(branch3[0],kernel_size=(1,1),strides=(1,1),padding='same',activation = 'linear',\n",
    "                        kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(weight_decay))(pathway_12)\n",
    "    if scale_residual: \n",
    "        x = Lambda(lambda p: p * 0.1)(x)\n",
    "    return add([x,pathway_12])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loving-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_input):\n",
    "    # stem\n",
    "    x = create_stem(img_input,concat_axis=CONCAT_AXIS)\n",
    "    # 5 x inception_A\n",
    "    for _ in range(1):\n",
    "        x=inception_A(x,params=[(128,),(64,128),(64,128,256),(512,)],concat_axis=CONCAT_AXIS)\n",
    "    # reduce A\n",
    "    x=reduce_A(x,params=[(512,),(256,256,512)],concat_axis=CONCAT_AXIS) # 768\n",
    "    # 10 x inception_B\n",
    "    for _ in range(1):\n",
    "        x=inception_B(x,params=[(1024,),(256,512,1024),(1536,)],concat_axis=CONCAT_AXIS)\n",
    "    # reduce B\n",
    "    x=reduce_B(x,params=[(125,256),(256,512),(128,512,768)],concat_axis=CONCAT_AXIS) # 1280\n",
    "    # 5 x inception_C\n",
    "    for _ in range(1):\n",
    "        x=inception_C(x,params=[(756,),(512,1024,2048),(3072,)],concat_axis=CONCAT_AXIS)\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    x=Dropout(DROPOUT)(x)\n",
    "    x = Dense(num_classes,activation='softmax',kernel_initializer=\"he_normal\",\n",
    "              kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial-tennessee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\darkenv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 7), strides=(1, 1), padding=\"same\", data_format=\"channels_last\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg..., use_bias=False)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Asus\\Anaconda3\\envs\\darkenv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (7, 1), strides=(1, 1), padding=\"same\", data_format=\"channels_last\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg..., use_bias=False)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Asus\\Anaconda3\\envs\\darkenv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1024, (1, 3), strides=(1, 1), padding=\"same\", data_format=\"channels_last\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg..., use_bias=False)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Asus\\Anaconda3\\envs\\darkenv\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2048, (3, 1), strides=(1, 1), padding=\"same\", data_format=\"channels_last\", kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg..., use_bias=False)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 32) 896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 32) 9248        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 18496       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  73856       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 128)  512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 128)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 64)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 192)  0           activation_5[0][0]               \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   12352       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 128)  24704       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 128)  73856       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 256)  295168      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 384)  0           activation_8[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  442496      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 384)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 512)  0           activation_11[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 64)   32832       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 64)   32832       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  73856       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 128)  65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 128)  73856       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 256)  295168      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 256)  1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 256)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 512)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 512)  262656      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 512)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 256)  131328      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 256)  590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 256)  1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 256)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 512)    2359808     add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 512)    1180160     activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 512)    2048        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 512)    2048        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 512)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 512)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 1536)   0           activation_18[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    393472      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 256)    1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 256)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 512)    917504      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 512)    2048        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 512)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 1024)   1573888     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 1024)   3670016     activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 1024)   4096        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 1024)   4096        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 1024)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 2048)   0           activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 1536)   3147264     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 1536)   0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 128)    196736      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 128)    512         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 128)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 125)    192125      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 256)    393472      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 512)    590336      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 125)    500         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 512)    2048        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 125)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 4, 256)    288256      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 4, 512)    1180160     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 768)    3539712     activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 256)    1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 4, 4, 768)    3072        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 512)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 768)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 1536)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 4, 3072)   0           activation_27[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 512)    1573376     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 512)    2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 512)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 4, 4, 1024)   1572864     activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 4, 4, 1024)   4096        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 756)    2323188     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 4, 2048)   6291456     activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 4, 4, 756)    3024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 4, 4, 2048)   8192        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 756)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 2048)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 4, 4, 2804)   0           activation_33[0][0]              \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 4, 3072)   8616960     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 3072)   0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 3072)         0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3072)         0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            6146        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 42,642,519\n",
      "Trainable params: 42,616,309\n",
      "Non-trainable params: 26,210\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE=256\n",
    "img_input=Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "output = create_model(img_input)\n",
    "model=Model(img_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-services",
   "metadata": {},
   "source": [
    "# RPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "contained-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-export",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chemical-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n",
    "\n",
    "    \n",
    "    input_shape = (num_rois,7,7,512)\n",
    "\n",
    "    pooling_regions = 7\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(1024, activation='relu', name='fc1'))(out)\n",
    "    out = TimeDistributed(Dropout(0.2))(out)\n",
    "    out = TimeDistributed(Dense(256, activation='relu', name='fc2'))(out)\n",
    "    out = TimeDistributed(Dropout(0.2))(out)\n",
    "\n",
    "    # There are two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-found",
   "metadata": {},
   "source": [
    "### IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sexual-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(au, bu, area_intersection):\n",
    "    area_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "    area_union = area_a + area_b - area_intersection\n",
    "    return area_union\n",
    "\n",
    "\n",
    "def intersection(ai, bi):\n",
    "    x = max(ai[0], bi[0])\n",
    "    y = max(ai[1], bi[1])\n",
    "    w = min(ai[2], bi[2]) - x\n",
    "    h = min(ai[3], bi[3]) - y\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    return w*h\n",
    "def iou(a, b):\n",
    "    # a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "        return 0.0\n",
    "\n",
    "    area_i = intersection(a, b)\n",
    "    area_u = union(a, b, area_i)\n",
    "\n",
    "    return float(area_i) / float(area_u + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n",
    "    \n",
    "    downscale = float(C.rpn_stride) \n",
    "    anchor_sizes = C.anchor_box_scales   # 128, 256, 512\n",
    "    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n",
    "    num_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
    "\n",
    "    # calculate the output map size based on the network architecture\n",
    "    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n",
    "\n",
    "    n_anchratios = len(anchor_ratios)    # 3\n",
    "    \n",
    "    # initialise empty output objectives\n",
    "    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
    "\n",
    "    num_bboxes = len(img_data['bboxes'])\n",
    "\n",
    "    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "    best_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
    "    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "    \n",
    "    gta = np.zeros((num_bboxes, 4))\n",
    "    for bbox_num, bbox in enumerate(img_data['bboxes']):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n",
    "        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
    "\n",
    "# rpn ground truth\n",
    "\n",
    "    for anchor_size_idx in range(len(anchor_sizes)):\n",
    "        for anchor_ratio_idx in range(n_anchratios):\n",
    "            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
    "            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n",
    "            \n",
    "            for ix in range(output_width):\n",
    "                # x-coordinates of the current anchor box\n",
    "                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n",
    "                \n",
    "                # ignore boxes that go across image boundaries\n",
    "                if x1_anc < 0 or x2_anc > resized_width:\n",
    "                    continue\n",
    "                    \n",
    "                for jy in range(output_height):\n",
    "\n",
    "                    # y-coordinates of the current anchor box\n",
    "                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "\n",
    "                    # ignore boxes that go across image boundaries\n",
    "                    if y1_anc < 0 or y2_anc > resized_height:\n",
    "                        continue\n",
    "\n",
    "                    bbox_type = 'neg'\n",
    "\n",
    "\n",
    "                    best_iou_for_loc = 0.0\n",
    "\n",
    "                    for bbox_num in range(num_bboxes):\n",
    "\n",
    "                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],\n",
    "                                       [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "        for idx in range(num_anchors_for_bbox.shape[0]):\n",
    "            if num_anchors_for_bbox[idx] == 0:\n",
    "            # no box with an IOU greater than zero ...\n",
    "            if best_anchor_for_bbox[idx, 0] == -1:\n",
    "                continue\n",
    "            y_is_box_valid[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            y_rpn_overlap[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            start = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n",
    "            y_rpn_regr[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
    "\n",
    "        y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "        y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "\n",
    "        y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "        y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "                        \n",
    "        num_regions = \n",
    "\n",
    "        if len(pos_locs[0]) > num_regions/2:\n",
    "            val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
    "            y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "            num_pos = num_regions/2\n",
    "\n",
    "        if len(neg_locs[0]) + num_pos > num_regions:\n",
    "            val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "            y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "        y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "        y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "        return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos\n",
    "    \n",
    "    \n",
    "lambda_rpn_regr = 1.0\n",
    "lambda_rpn_class = 1.0\n",
    "\n",
    "lambda_cls_regr = 1.0\n",
    "lambda_cls_class = 1.0\n",
    "\n",
    "epsilon = 1e-4\n",
    "\n",
    "def rpn_loss_cls(num_anchors):\n",
    "\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "\n",
    "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "def rpn_loss_cls(num_anchors):\n",
    "\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "\n",
    "            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "def class_loss_regr(num_classes):\n",
    "\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
    "        x_abs = K.abs(x)\n",
    "        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n",
    "        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n",
    "    return class_loss_regr_fixed_num\n",
    "\n",
    "\n",
    "def class_loss_cls(y_true, y_pred):\n",
    "    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-valve",
   "metadata": {},
   "source": [
    "## RPN to ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
    "\n",
    "    regr_layer = regr_layer / C.std_scaling\n",
    "\n",
    "    anchor_sizes = C.anchor_box_scales   # (3 in here)\n",
    "    anchor_ratios = C.anchor_box_ratios  # (3 in here)\n",
    "\n",
    "    assert rpn_layer.shape[0] == 1\n",
    "\n",
    "    (rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "    curr_layer = 0\n",
    "\n",
    "    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "    for anchor_size in anchor_sizes:\n",
    "        for anchor_ratio in anchor_ratios:\n",
    "            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "            anchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n",
    "            anchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n",
    "            \n",
    "\n",
    "            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] \n",
    "            regr = np.transpose(regr, (2, 0, 1))\n",
    "\n",
    "            X, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "            # Calculate anchor position and size for each feature map point\n",
    "            A[0, :, :, curr_layer] = X - anchor_x/2 \n",
    "            A[1, :, :, curr_layer] = Y - anchor_y/2 \n",
    "            A[2, :, :, curr_layer] = anchor_x       \n",
    "            A[3, :, :, curr_layer] = anchor_y       \n",
    "\n",
    "            # Apply regression to x, y, w and h if there is rpn regression layer\n",
    "            if use_regr:\n",
    "                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "            # Avoid width and height exceeding 1\n",
    "            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "            # Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "            # x1, y1 is top left coordinate\n",
    "            # x2, y2 is bottom right coordinate\n",
    "            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "            # Avoid bboxes drawn outside the feature map\n",
    "            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "            A[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
    "\n",
    "            curr_layer += 1\n",
    "\n",
    "    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n",
    "\n",
    "    x1 = all_boxes[:, 0]\n",
    "    y1 = all_boxes[:, 1]\n",
    "    x2 = all_boxes[:, 2]\n",
    "    y2 = all_boxes[:, 3]\n",
    "\n",
    "    # Find out the bboxes which is illegal and delete them from bboxes list\n",
    "    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "    all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "    all_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "\n",
    "    result = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-limitation",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network Inception\n",
    "shared_layers = nn_base(img_input, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\n",
    "rpn = rpn_layer(shared_layers, num_anchors)\n",
    "\n",
    "classifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "# Because the google colab can only run the session several hours one time (then you need to connect again), \n",
    "# we need to save the model and load the model to continue training\n",
    "if not os.path.isfile(C.model_path):\n",
    "    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n",
    "    try:\n",
    "        print('This is the first time of your training')\n",
    "        print('loading weights from {}'.format(C.base_net_weights))\n",
    "        model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "        model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "    except:\n",
    "        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "            https://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "    \n",
    "    # Create the record.csv file to record losses, acc and mAP\n",
    "    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
    "else:\n",
    "    # If this is a continued training, load the trained model from before\n",
    "    print('Continue training based on previous trained model')\n",
    "    print('Loading weights from {}'.format(C.model_path))\n",
    "    model_rpn.load_weights(C.model_path, by_name=True)\n",
    "    model_classifier.load_weights(C.model_path, by_name=True)\n",
    "    \n",
    "    # Load the records\n",
    "    record_df = pd.read_csv(record_path)\n",
    "\n",
    "    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
    "    r_class_acc = record_df['class_acc']\n",
    "    r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
    "    r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
    "    r_loss_class_cls = record_df['loss_class_cls']\n",
    "    r_loss_class_regr = record_df['loss_class_regr']\n",
    "    r_curr_loss = record_df['curr_loss']\n",
    "    r_elapsed_time = record_df['elapsed_time']\n",
    "    r_mAP = record_df['mAP']\n",
    "\n",
    "    print('Already train %dK batches'% (len(record_df)))\n",
    "    \n",
    "    \n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "# Training setting\n",
    "total_epochs = len(record_df)\n",
    "r_epochs = len(record_df)\n",
    "\n",
    "epoch_length = 1000\n",
    "num_epochs = 2   #Just of sharing the karnel running with 2 epoch , you try with min 20 epochs\n",
    "iter_num = 0\n",
    "\n",
    "total_epochs += num_epochs\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "\n",
    "if len(record_df)==0:\n",
    "    best_loss = np.Inf\n",
    "else:\n",
    "    best_loss = np.min(r_curr_loss)\n",
    "    \n",
    "start_time = time.time()\n",
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "    progbar = generic_utils.Progbar(epoch_length)\n",
    "    print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n",
    "    \n",
    "    r_epochs += 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "                rpn_accuracy_rpn_monitor = []\n",
    "#                 print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "                if mean_overlapping_bboxes == 0:\n",
    "                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "            # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n",
    "            X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)\n",
    "\n",
    "            # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]\n",
    "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "\n",
    "            # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n",
    "            P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "            # R: bboxes (shape=(300,4))\n",
    "            # Convert rpn layer to roi bboxes\n",
    "            R = rpn_to_roi(P_rpn[0], P_rpn[1], C, K.common.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "\n",
    "            X2, Y1, Y2, IouS = calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "            # If X2 is None means there are no matching bboxes\n",
    "            if X2 is None:\n",
    "                rpn_accuracy_rpn_monitor.append(0)\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Find out the positive anchors and negative anchors\n",
    "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "            if len(neg_samples) > 0:\n",
    "                neg_samples = neg_samples[0]\n",
    "            else:\n",
    "                neg_samples = []\n",
    "\n",
    "            if len(pos_samples) > 0:\n",
    "                pos_samples = pos_samples[0]\n",
    "            else:\n",
    "                pos_samples = []\n",
    "\n",
    "            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "            if C.num_rois > 1:\n",
    "                # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n",
    "                if len(pos_samples) < C.num_rois//2:\n",
    "                    selected_pos_samples = pos_samples.tolist()\n",
    "                else:\n",
    "                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "                \n",
    "                # Randomly choose (num_rois - num_pos) neg samples\n",
    "                try:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                except:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "                \n",
    "                # Save all the pos and neg samples in sel_samples\n",
    "                sel_samples = selected_pos_samples + selected_neg_samples\n",
    "            else:\n",
    "                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "                selected_neg_samples = neg_samples.tolist()\n",
    "                if np.random.randint(0, 2):\n",
    "                    sel_samples = random.choice(neg_samples)\n",
    "                else:\n",
    "                    sel_samples = random.choice(pos_samples)\n",
    "\n",
    "                    \n",
    "            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "            losses[iter_num, 0] = loss_rpn[1]\n",
    "            losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            losses[iter_num, 2] = loss_class[1]\n",
    "            losses[iter_num, 3] = loss_class[2]\n",
    "            losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "            iter_num += 1\n",
    "\n",
    "            progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                      ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "            if iter_num == epoch_length:\n",
    "                loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                loss_class_cls = np.mean(losses[:, 2])\n",
    "                loss_class_regr = np.mean(losses[:, 3])\n",
    "                class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                rpn_accuracy_for_epoch = []\n",
    "\n",
    "                if C.verbose:\n",
    "                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                    print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                    print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n",
    "                    print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "                    elapsed_time = (time.time()-start_time)/60\n",
    "\n",
    "                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                iter_num = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "                if curr_loss < best_loss:\n",
    "                    if C.verbose:\n",
    "                        print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                    best_loss = curr_loss\n",
    "                    model_all.save_weights(C.model_path)\n",
    "\n",
    "                new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n",
    "                           'class_acc':round(class_acc, 3), \n",
    "                           'loss_rpn_cls':round(loss_rpn_cls, 3), \n",
    "                           'loss_rpn_regr':round(loss_rpn_regr, 3), \n",
    "                           'loss_class_cls':round(loss_class_cls, 3), \n",
    "                           'loss_class_regr':round(loss_class_regr, 3), \n",
    "                           'curr_loss':round(curr_loss, 3), \n",
    "                           'elapsed_time':round(elapsed_time, 3), \n",
    "                           'mAP': 0}\n",
    "\n",
    "                record_df = record_df.append(new_row, ignore_index=True)\n",
    "                record_df.to_csv(record_path, index=0)\n",
    "\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception: {}'.format(e))\n",
    "            continue\n",
    "\n",
    "print('Training complete, exiting.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
